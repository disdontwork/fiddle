import React, { useEffect, useRef, useState } from 'react';
import { Camera } from 'lucide-react';
import * as tf from '@tensorflow/tfjs';
import * as handpose from '@tensorflow-models/handpose';

const NOTE_FREQUENCIES = {
  'C4': 261.63,
  'E4': 329.63,
  'G4': 392.00,
  'C5': 523.25
};

const HandTheremin = () => {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const audioContextRef = useRef(null);
  const oscillatorRef = useRef(null);
  const chordOscillatorsRef = useRef([]);
  const [isPlaying, setIsPlaying] = useState(false);
  const [model, setModel] = useState(null);

  useEffect(() => {
    const initializeHandpose = async () => {
      const loadedModel = await handpose.load();
      setModel(loadedModel);
    };

    if (isPlaying) {
      initializeHandpose();
      initializeAudio();
      initializeCamera();
    }

    return () => cleanup();
  }, [isPlaying]);

  const initializeAudio = () => {
    audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
    
    // Main oscillator
    oscillatorRef.current = audioContextRef.current.createOscillator();
    const gainNode = audioContextRef.current.createGain();
    gainNode.gain.value = 0;
    oscillatorRef.current.connect(gainNode);
    gainNode.connect(audioContextRef.current.destination);
    oscillatorRef.current.start();

    // Chord oscillators
    chordOscillatorsRef.current = Object.values(NOTE_FREQUENCIES).map(freq => {
      const osc = audioContextRef.current.createOscillator();
      const gain = audioContextRef.current.createGain();
      gain.gain.value = 0;
      osc.frequency.value = freq;
      osc.connect(gain);
      gain.connect(audioContextRef.current.destination);
      osc.start();
      return { oscillator: osc, gain };
    });
  };

  const initializeCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480 }
      });
      
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }

      detectHands();
    } catch (error) {
      console.error('Camera initialization error:', error);
    }
  };

  const detectHands = async () => {
    if (!model || !videoRef.current || !canvasRef.current) return;

    const predictions = await model.estimateHands(videoRef.current);
    const ctx = canvasRef.current.getContext('2d');
    
    ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
    ctx.drawImage(videoRef.current, 0, 0, canvasRef.current.width, canvasRef.current.height);

    predictions.forEach((hand, index) => {
      const palm = hand.annotations.palmBase[0];
      drawHand(ctx, hand, index);

      // Use z-coordinate (depth) for audio control
      const depth = palm[2];
      if (index === 0) { // Right hand controls pitch
        const frequency = mapDepthToFrequency(depth);
        oscillatorRef.current.frequency.setValueAtTime(
          frequency,
          audioContextRef.current.currentTime
        );
      } else { // Left hand controls chord volume
        const volume = mapDepthToVolume(depth);
        chordOscillatorsRef.current.forEach(({ gain }) => {
          gain.gain.setValueAtTime(
            volume,
            audioContextRef.current.currentTime
          );
        });
      }
    });

    if (isPlaying) {
      requestAnimationFrame(detectHands);
    }
  };

  const drawHand = (ctx, hand, index) => {
    const landmarks = hand.landmarks;
    landmarks.forEach(point => {
      ctx.beginPath();
      ctx.arc(point[0], point[1], 5, 0, 2 * Math.PI);
      ctx.fillStyle = index === 0 ? '#FF0000' : '#00FF00';
      ctx.fill();
    });
  };

  const mapDepthToFrequency = (depth) => {
    const minFreq = 220;
    const maxFreq = 880;
    return minFreq + (1 - (depth + 1) / 2) * (maxFreq - minFreq);
  };

  const mapDepthToVolume = (depth) => {
    return Math.max(0, 0.5 - (depth + 1) / 2);
  };

  const cleanup = () => {
    if (videoRef.current?.srcObject) {
      videoRef.current.srcObject.getTracks().forEach(track => track.stop());
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
  };

  const togglePlaying = () => {
    setIsPlaying(!isPlaying);
  };

  return (
    <div className="flex flex-col items-center gap-4 p-4">
      <button 
        onClick={togglePlaying}
        className="flex items-center gap-2 px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600"
      >
        <Camera size={24} />
        {isPlaying ? 'Stop' : 'Start'} Theremin
      </button>
      
      {isPlaying && (
        <div className="relative w-full max-w-2xl">
          <video
            ref={videoRef}
            className="hidden"
            autoPlay
            playsInline
          />
          <canvas
            ref={canvasRef}
            width={640}
            height={480}
            className="w-full border rounded"
          />
          <div className="mt-4 text-center text-sm text-gray-600">
            Move your right hand closer/further to control pitch
            <br />
            Move your left hand closer/further to control chord volume
          </div>
        </div>
      )}
    </div>
  );
};

export default HandTheremin;
